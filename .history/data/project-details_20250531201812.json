{
  "End-to-End Data Engineering Journey": {
    "title": "End-to-End Data Engineering Journey",
    "image": "..",
    "tags": [
      "Python",
      "SQL",
      "ETL",
      "Orchestration ETL",
      "Pipeline Optimization",
      "Data Storage Layer",
      "Data Modeling",
      "Infrastructure",
      "Bash",
      "Shell",
      "Docker",
      "Portainer",
      "Data Security",
      "Server Setup"
    ],
    "content": [
      {
        "type": "text",
        "value": "My End-to-End Data Engineering Journey"
      },
      {
        "type": "text",
        "value": "Background & Motivation"
      },
      {
        "type": "text",
        "value": "Upon joining this project, I encountered a significant challenge—organizational data was highly fragmented across multiple sources. The existing infrastructure lacked any centralized or automated data pipeline. Most data remained siloed within disparate systems and was primarily analyzed through manually maintained Excel files, leading to underutilization of valuable insights."
      },
       {
        "type": "text",
        "value": "At that time, the organization was just beginning to explore data-driven practices. Our data team consisted of only two members—my supervisor, who had initiated some foundational work, and myself. Given the lean team structure, I was expected not only to support but also to independently manage and understand the system from the ground up. This led me to dive into designing and implementing core data infrastructure components."
      },
      {
        "type": "text",
        "value": "The first step was to develop a high-level data domain mapping, classifying datasets into key functional domains. I implemented a color-coded system to visually communicate the current integration status of each domain:"
      },
      {
        "type": "text",
        "value": "🟩 Green: Fully integrated and available via the data mart"
      },
      {
        "type": "text",
        "value": "🟨 Yellow: In progress"
      },
      {
        "type": "text",
        "value": "⬜ Gray: Not yet available"
      },
      {
        "type": "text",
        "value": "This domain overview helped align stakeholders and clarify which systems had been onboarded into our centralized database, ready for BAU (Business-As-Usual) access."
      },
      {
        "type": "text",
        "value": "Recognizing the scale and impact of the data fragmentation, I set out to build an end-to-end data infrastructure—from ingestion, transformation, and data modeling to reporting and monitoring. My objective was to deliver:"
      },
      {
        "type": "text",
        "value": "- A scalable, automated data pipeline \n - Clean, reliable data for business teams \n - Reduced manual data wrangling \n - Accelerated time-to-insight for decision-makers"
      },
      {
        "type": "text",
        "value": "This foundational work laid the groundwork for the subsequent project titled [Project Name Here], which formalized these efforts into a sustainable data engineering framework."
      },
      {
        "type": "image",
        "value": "/images/data-en-1.jpg",
        "caption": "Domain"
      },
      {
        "type": "text",
        "value": "Phase 1: Building from Scratch – Ubuntu Server & Database Foundation \n\n The project began with a blank slate. I collaborated closely with the Infrastructure team to provision and configure an on-premises Ubuntu Server. This phase involved setting up core system configurations, including: \n\n - Network and Security Setup: Configuring firewall rules, SSH access, and port forwarding \n- OS Installation: Installing and hardening the Ubuntu Server (Linux) environment \n - Backup Policy Compliance: Aligning with organizational standards for data backup and recovery procedures \n\n To organize system services effectively, we adopted a container-based architecture using Portainer, which allowed us to manage and segment different stacks of services in a modular and maintainable way."
      }      ,
      {
        "type": "text",
        "value": "       Database Setup\n\nAs a foundational layer, we deployed a MySQL relational database to handle raw and transactional data ingestion. MySQL was selected based on its performance efficiency for high-frequency transaction logging—especially suitable for IoT-originated datasets such as sensor feeds and device logs."
      },
      {
        "type": "text",
        "value": "This database served as the Raw / Staging Layer, capturing incoming data streams for further processing."
      },
      {
        "type": "text",
        "value": "To facilitate schema design, database maintenance, and query execution, I used DBeaver, a powerful database GUI tool, to perform:\n\n- DDL scripting (table/view creation, indexing)\n- SQL query writing for exploration and validation\n- Connection management across different environments (Dev/Test/Prod)\n\nThis initial phase laid the groundwork for the subsequent development of ETL pipelines and data-transformation workflows in the next phase."
      },
      {
        "type": "image",
        "value": "/images/example.png",
        "caption": "Example.png"
      },
      {
        "type": "text",
        "value": "     Phase 2: Designing a Scalable ETL Pipeline with Windmill\n\nTo establish a reliable and maintainable data pipeline, I chose Windmill as the orchestration tool for building ETL workflows. Given the current system’s moderate complexity and relatively low data volume, Windmill offered an ideal balance between functionality and simplicity. Its intuitive UI/UX and low operational overhead made it a pragmatic choice over more complex orchestrators such as Apache Airflow, which is better suited for large-scale systems and may be considered for future expansion."
      },
      {
        "type": "text",
        "value": "       Rationale Behind Tool Selection\n\n- Low learning curve for IT and Dev teams with limited experience in complex orchestration tools\n- Efficient UI for managing scripts, scheduling, and variables\n- Built-in integrations and error tracking, which reduce setup time for small- to mid-sized data workflows"
      },
      {
        "type": "text",
        "value": "       ETL Architecture Overview\n\nThe pipeline was modularized into three core stages:\n\n- Extract: Ingest data from internal and external systems (e.g., APIs, IoT devices, vendor systems)\n- Transform: Clean, normalize, deduplicate, and enrich data to meet business-logic requirements\n- Load: Push the processed data into the target relational database, structured for downstream analytics"
      },
      {
        "type": "text",
        "value": "       Extract\n\nData extraction involved both technical and organizational challenges. Each integration required:\n\n- Engaging Data Owners to gain consent and clarify business rationale\n- Collaborating with internal and external stakeholders, including vendors\n- Implementing technical connectors via RESTful APIs, webhooks, or file-based ingestion\n- Handling network configurations and authentication, often in partnership with the infrastructure team"
      },
      {
        "type": "text",
        "value": "       Transform\n\nTransformations ranged from simple data formatting to more complex logic encoding business rules. Since our end users consumed data primarily via Power BI, the goal was to minimize complex DAX expressions and instead deliver analysis-ready data via a curated Data Mart layer.\n\nThis ensured:\n\n- Faster report development\n- Improved data literacy among business users\n- Reduced dependency on BI developers for logic implementation"
      },
      {
        "type": "text",
        "value": "       Load\n\nThe final phase included various data-loading strategies, depending on use case:\n\n- Full replacement for static datasets\n- Incremental updates using timestamp logic or unique keys\n- Chunk/batch processing for large datasets to avoid memory bottlenecks\n\nEach script was modularized by purpose to support scalability and debugging—for example:\n\n- Full refresh\n- Point updates\n- Append-only loads"
      },
      {
        "type": "text",
        "value": "       Scheduler & Monitoring\n\nWindmill’s built-in scheduling (via cron syntax) enabled automatic execution of pipelines at specified intervals. Additionally, I implemented error notifications via email to ensure that any pipeline failures could be quickly identified and resolved by the team."
      },
      {
        "type": "text",
        "value": "       Security & Credential Management\n\nTo ensure secure and auditable workflows:\n\n- IAM credentials and API keys were stored as environment variables and encrypted secrets\n- These were managed via Windmill’s secure vault, ensuring clear separation between business logic and sensitive configuration\n- This allowed safe collaboration without exposing privileged information in scripts or version control"
      },
      {
        "type": "text",
        "value": "     Phase 3: Building the Data Mart for Analytics Consumption\n\nFollowing data ingestion into the central database, the next critical step was the creation of curated Data Marts—purpose-built datasets tailored to the analytical and reporting needs of various business units."
      },
      {
        "type": "text",
        "value": "These data marts were designed to serve as the semantic layer for self-service BI tools, reducing the need for business users to query raw tables or write complex transformation logic."
      },
      {
        "type": "text",
        "value": "       Governance & Security Considerations\n\nThis phase required careful attention to two key areas:\n\n1. Data Governance\n2. Visualization & Insight Delivery (via Power BI)"
      },
      {
        "type": "text",
        "value": "Given that some source data contained Personally Identifiable Information (PII), it was essential to adhere to data privacy best practices. My approach included:\n\n- Segregation of Sensitive Data: All PII was isolated from analytical layers\n- Anonymization & Encoding: Sensitive attributes were transformed using techniques such as one-hot encoding, ensuring analytical utility without exposing identity\n- Marking and Classification: Governance tags were applied for audit and policy enforcement (as part of a future expansion)"
      },
      {
        "type": "text",
        "value": "As a result, the finalized Data Marts were secure, anonymized, and analytics-ready, ensuring compliance and usability."
      },
      {
        "type": "text",
        "value": "       Architecture & Performance Design\n\nUsing Windmill, I structured the transformation pipeline as modular and asynchronous flows, allowing for:\n\n- Parallel Processing: Independent transformation blocks executed concurrently to reduce overall ETL runtime\n- Nested Sub-flows: Each task in the flow could call sub-flows to encapsulate reusable logic (e.g., shared cleaning steps or lookups)\n- Reduced Complexity: Visual flow diagrams improved maintainability and onboarding for new team members"
      },
      {
        "type": "text",
        "value": "This design not only increased pipeline efficiency but also provided clear visualization of each step in the data flow, from staging through to mart population."
      }      ,
      {
        "type": "text",
        "value": "       BI Integration\n\nThe Data Mart output served as the primary data source for Power BI dashboards, which I designed to:\n\n- Simplify user experience (minimize reliance on complex DAX logic)\n- Support drill-down, trend analysis, and KPI tracking\n- Enable data-driven decision-making across departments"
      },
      {
        "type": "text",
        "value": "By ensuring that data delivered to Power BI was clean, enriched, and pre-aggregated, we significantly reduced time-to-insight and empowered business users to explore data confidently."
      },
      {
        "type": "text",
        "value": "     Phase 4: Visualization & Insight Delivery\n\nThe final phase focused on enabling business users to independently access, explore, and derive value from data. This phase was separated into two core initiatives:\n\n- Dashboard & Monitoring\n- Reporting & Advanced Analytics"
      },
      {
        "type": "text",
        "value": "These efforts were part of a broader initiative under the project \"[Project Name Here]\", designed to empower BAU (Business-As-Usual) teams to leverage BI tools effectively and reduce operational dependency on the Business Analytics team."
      },
      {
        "type": "text",
        "value": "       Dashboard & Monitoring\n\nI connected the PostgreSQL Data Mart to Power BI, creating a suite of dashboards tailored to business needs. Key features included:\n\n- Multi-dimensional analysis (daily, weekly, monthly trends)\n- User-friendly design with clear navigation and visual cues\n- Pre-built DAX measures for insights such as:\n    - Growth Rate comparisons\n    - Top-N performance by segment\n    - Year-over-year analysis"
      },
      {
        "type": "text",
        "value": "To support scalability and adoption:\n\n- Dashboards were designed for low-code/no-code handoff to BAU departments\n- Initial deployment included training and onboarding for designated users\n- Long-term ownership was transitioned to each department, who could extend and modify dashboards based on evolving needs"
      },
      {
        "type": "text",
        "value": "       Reporting & Deep-Dive Analysis\n\nFor more complex analytical needs, especially those requiring cross-domain joins or hypothesis testing beyond available Data Mart dimensions, responsibility remained with the Business Analytics team. These “deep reports” addressed:\n\n- Ad hoc strategic questions\n- Trend anomalies\n- Exploratory insights for decision-makers"
      },
      {
        "type": "text",
        "value": "The backend pipeline supported this by integrating cleaned data from multiple domains into a semantic model within Power BI, enhancing accessibility and cross-functional analysis."
      },
      {
        "type": "text",
        "value": "       Governance & Access Control\n\nTo ensure secure and organized usage:\n\n- Each department was assigned a dedicated Power BI Workspace\n- Role-based access was enforced via permission controls aligned with the organization’s Data Catalog\n- Sensitive dashboards and datasets were isolated as needed to comply with internal governance standards"
      },
      {
        "type": "text",
        "value": "       Summary\n\nBy establishing a seamless connection from PostgreSQL to Power BI, I enabled departments to make data-driven decisions using self-service dashboards. These efforts not only improved data visibility but also:\n\n- Reduced turnaround time for insights\n- Offloaded routine reporting tasks from the analytics team\n- Increased data literacy across business units"
      },
      {
        "type": "text",
        "value": "Note: The design, implementation process, and outcomes of selected dashboards are documented in a dedicated sub-project."
      }      ,
      {
        "type": "text",
        "value": "     Challenges & Deep-Dive Learnings\n\nThroughout the course of this project, I encountered a wide range of technical and analytical challenges that significantly contributed to my professional growth. These real-world scenarios required both hands-on problem-solving and cross-domain collaboration. Key learning areas included:"
      },
      {
        "type": "text",
        "value": "       System Configuration & Troubleshooting\n\n- Gained extensive experience working with Linux CLI, particularly in debugging environment setup and deployment errors.\n- Resolved issues related to:\n    - Misconfigured `YAML` files in Windmill and Docker stacks\n    - Networking challenges within Portainer, ensuring proper container-to-container communication\n    - Port conflicts and version compatibility across multiple services"
      },
      {
        "type": "text",
        "value": "       ETL Pipeline Debugging\n\n- Investigated root causes of pipeline failures across the stack:\n    - Source-side issues (e.g., data quality, schema mismatches)\n    - Credential mismanagement\n    - Execution-level errors within Windmill workflows\n- Designed alert mechanisms and fallback strategies to reduce downtime and improve recovery time"
      },
      {
        "type": "text",
        "value": "       Query & Workflow Optimization\n\n- Improved performance and reliability by:\n    - Optimizing complex SQL queries and Python scripts\n    - Redesigning pipeline logic to reduce memory and processing overhead\n    - Implementing batch updates, error handling, and retry mechanisms for unstable data sources"
      },
      {
        "type": "text",
        "value": "       Business Logic Reconstruction & Data Investigation\n\n- Dealt with legacy datasets stored without clear documentation\n- Conducted reverse engineering of table relationships and column semantics to rediscover business logic\n- Collaborated with stakeholders to reconstruct transformation rules and validate data accuracy before delivering to BI tools"
      },
      {
        "type": "text",
        "value": "These experiences deepened my ability to work across the full data lifecycle, from system-level configuration to business-impactful analytics, and strengthened my confidence in designing scalable, secure, and business-aligned data solutions."
      }      ,
      {
        "type": "text",
        "value": "     Final Outcome & Personal Reflection"
      },
      {
        "type": "text",
        "value": "       Final Outcome\n\nBy the end of this project, I successfully delivered a functioning end-to-end data infrastructure that transformed fragmented, underutilized data into a structured, accessible, and business-ready asset. Key achievements included:"
      },
      {
        "type": "text",
        "value": "- ✅ Deployment of an on-premises Ubuntu server with secure, containerized services using Portainer\n- ✅ Design and implementation of a modular ETL pipeline using Windmill to extract, clean, and load data into MySQL/PostgreSQL\n- ✅ Construction of data marts with clear separation of PII, aligned with data governance standards\n- ✅ Integration with Power BI, enabling departments to access self-service dashboards and perform actionable analysis\n- ✅ Reduction in manual reporting workload for the Business Analytics team through automation and empowerment of BAU users"
      },
      {
        "type": "text",
        "value": "These outcomes contributed to a data-driven culture within the organization, increasing transparency, improving decision-making speed, and enhancing collaboration between technical and business teams."
      },
      {
        "type": "text",
        "value": "       Personal Reflection\n\nThis project was not only a technical challenge but also a significant milestone in my personal development as a data professional. Key reflections include:"
      },
      {
        "type": "text",
        "value": "- I gained   end-to-end ownership experience  , from server provisioning and data architecture to dashboard deployment and user enablement.\n- I learned to navigate ambiguity, particularly when working with undocumented legacy data or undefined business rules.\n- I strengthened my skills in problem-solving under constraints, especially in a small team environment where flexibility and proactivity were essential.\n- I developed a deeper appreciation for data governance, user-centric design, and cross-functional collaboration."
      },
      {
        "type": "text",
        "value": "Most importantly, this experience affirmed my passion for solving real-world problems with data—balancing technical excellence with business value."
      }
    ]
  },
  "real-time-dashboard": {
    "title": "Real-Time Dashboard",
    "image": "../images/project-dashboard.png",
    "tags": [
      "Power BI",
      "Realtime",
      "Dashboard"
    ],
    "content": [
      {
        "type": "text",
        "value": "Built a real-time dashboard in Power BI connected to event streams."
      },
      {
        "type": "image",
        "value": "../images/realtime-dashboard-sample.png",
        "caption": "Live Dashboard Snapshot"
      }
    ]
  },
  "data-lake-integration": {
    "title": "Data Lake Integration",
    "image": "../images/project-datalake.png",
    "tags": [
      "IoT",
      "Data Lake",
      "Integration"
    ],
    "content": [
      {
        "type": "text",
        "value": "Connected multiple IoT data sources into a centralized data lake using NiFi and MinIO."
      }
    ]
  },
  "feedback-dashboard": {
    "title": "Customer Feedback Dashboard",
    "image": "../images/project-feedback.png",
    "tags": [
      "Survey",
      "Sentiment",
      "Dashboard"
    ],
    "content": [
      {
        "type": "text",
        "value": "Built Power BI dashboard to analyze customer satisfaction trends from surveys."
      }
    ]
  },
  "campaign-performance-analysis": {
    "title": "Campaign Performance Analysis",
    "image": "../images/project-campaign.png",
    "tags": [
      "Marketing",
      "A/B Test",
      "Budget Optimization"
    ],
    "content": [
      {
        "type": "text",
        "value": "Analyzed campaign variations using A/B testing to inform budget decisions."
      }
    ]
  },
  "operational-kpi-tracking": {
    "title": "Operational KPI Tracking",
    "image": "../images/project-kpi.png",
    "tags": [
      "KPI",
      "Sales",
      "Service"
    ],
    "content": [
      {
        "type": "text",
        "value": "Created sales and service performance dashboards for internal tracking."
      }
    ]
  },
  "bi-sales-performance": {
    "title": "Sales Performance Dashboard",
    "image": "../images/bi-sales-dashboard.png",
    "tags": [
      "Sales",
      "Power BI",
      "Dashboard"
    ],
    "content": [
      {
        "type": "text",
        "value": "Designed Power BI dashboards that provide sales KPI breakdowns by region, product line, and time."
      }
    ]
  },
  "bi-customer-segmentation": {
    "title": "Customer Segmentation Report",
    "image": "../images/bi-customer-segmentation.png",
    "tags": [
      "Segmentation",
      "Customer Behavior",
      "Analytics"
    ],
    "content": [
      {
        "type": "text",
        "value": "Clustered customers using RFM metrics and behavior patterns to drive personalized marketing strategies."
      }
    ]
  },
  "bi-kpi-overview": {
    "title": "Executive KPI Overview",
    "image": "../images/bi-kpi-overview.png",
    "tags": [
      "Executive",
      "Dashboard",
      "KPI"
    ],
    "content": [
      {
        "type": "text",
        "value": "Built summary dashboards for executives to monitor company-wide KPIs in real-time."
      }
    ]
  },
  "marketing-insight-framework": {
    "title": "Marketing Insight Framework",
    "image": "../images/data-strategy-marketing.png",
    "tags": [
      "Marketing",
      "Data-Driven",
      "Performance"
    ],
    "content": [
      {
        "type": "text",
        "value": "Built a strategic framework for analyzing marketing campaign effectiveness across platforms."
      }
    ]
  },
  "event-analytics-dashboard": {
    "title": "Event Analytics Dashboard",
    "image": "../images/data-strategy-event.png",
    "tags": [
      "IoT",
      "Analytics",
      "Event"
    ],
    "content": [
      {
        "type": "text",
        "value": "Analyzed real-time traffic and zone usage using IoT data and displayed results in Power BI."
      }
    ]
  },
  "decision-intelligence-system": {
    "title": "Decision Intelligence System",
    "image": "../images/data-strategy-decision.png",
    "tags": [
      "AI",
      "Automation",
      "Decision Support"
    ],
    "content": [
      {
        "type": "text",
        "value": "Developed logic flows and automated actions triggered by analytical thresholds and forecasts."
      }
    ]
  },
  "chatbot-analytics-pipeline": {
    "title": "Chatbot Analytics Pipeline",
    "image": "../images/data-product-chatbot.png",
    "tags": [
      "Chatbot",
      "ETL",
      "Datamart"
    ],
    "content": [
      {
        "type": "text",
        "value": "Extracted conversation logs and funneled data into Datamart to derive user engagement insights."
      }
    ]
  },
  "self-service-report-generator": {
    "title": "Self-Service Report Generator",
    "image": "../images/data-product-reportgen.png",
    "tags": [
      "SQL",
      "Power BI",
      "Automation"
    ],
    "content": [
      {
        "type": "text",
        "value": "Enabled internal users to generate reports dynamically from multiple databases using simple filters."
      }
    ]
  },
  "customer-feedback-hub": {
    "title": "Customer Feedback Hub",
    "image": "../images/data-product-feedback.png",
    "tags": [
      "Feedback",
      "Integration",
      "Customer Voice"
    ],
    "content": [
      {
        "type": "text",
        "value": "Centralized survey, support, and online reviews into a hub for product and service improvements."
      }
    ]
  },
"gcp-certification": {
  "title": "Google Cloud Certified Associate",
  "image": "../images/gcp-cert.png",
  "tags": ["GCP", "Cloud", "Infrastructure"],
  "content": [
    {
      "type": "text",
      "value": "Demonstrated foundational knowledge of Google Cloud services and architecture."
    },
    {
      "type": "image",
      "value": "../images/gcp-diagram.png",
      "caption": "GCP Service Overview"
    },
    {
      "type": "text",
      "value": "This certification affirms capability to manage cloud-native apps and services in GCP."
    }
  ]
},
"azure-certification": {
  "title": "Azure Fundamentals",
  "image": "../images/azure-cert.png",
  "tags": ["Azure", "Cloud", "Fundamentals"],
  "content": [
    {
      "type": "text",
      "value": "Certified on core Azure concepts, pricing models, and governance tools."
    },
    {
      "type": "image",
      "value": "../images/azure-overview.png",
      "caption": "Azure Cloud Foundations"
    },
    {
      "type": "text",
      "value": "This cert reflects understanding of Azure’s key services and cloud principles."
    }
  ]
},
"aws-certification": {
  "title": "AWS Solutions Architect Associate",
  "image": "../images/aws-cert.png",
  "tags": ["AWS", "Cloud", "Architecture"],
  "content": [
    {
      "type": "text",
      "value": "Designed scalable cloud solutions following AWS best practices."
    },
    {
      "type": "image",
      "value": "../images/aws-diagram.png",
      "caption": "AWS Architecture"
    },
    {
      "type": "text",
      "value": "This certification validates expertise in deploying secure cloud solutions."
    }
  ]
}
}