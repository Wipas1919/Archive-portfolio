{
  "End-to-End Data Engineering Journey": {
    "title": "End-to-End Data Engineering Journey",
    "image": "..",
    "tags": [
      "Python",
      "SQL",
      "ETL",
      "Orchestration ETL",
      "Pipeline Optimization",
      "Data Storage Layer",
      "Data Modeling",
      "Infrastructure",
      "Bash",
      "Shell",
      "Docker",
      "Portainer",
      "Data Security",
      "Server Setup"
    ],
    "content": [
      {
        "type": "image",
        "value": "/images/data-en-2.jpg",
        "caption": "Domain"
      },
      {
        "type": "text",
        "value": "<strong>Background & Motivation</strong>"
      },
      {
        "type": "text",
        "class": "indented",
        "value": "„Ö§„Ö§Upon joining this project, I encountered a significant challenge‚Äîorganizational data was highly fragmented across multiple sources. The existing infrastructure lacked any centralized or automated data pipeline. Most data remained siloed within disparate systems and was primarily analyzed through manually maintained Excel files, leading to underutilization of valuable insights."
      },
       {
        "type": "text",
        "value": "<b>„Ö§„Ö§At that time, the organization was just beginning to explore data-driven practices. Our data team consisted of only two members‚Äîmy supervisor, who had initiated some foundational work, and myself. Given the lean team structure, I was expected not only to support but also to independently manage and understand the system from the ground up. This led me to dive into designing and implementing core data infrastructure components.<b>"
      },
      {
        "type": "text",
        "value": "„Ö§„Ö§The first step was to develop a high-level data domain mapping, classifying datasets into key functional domains. I implemented a color-coded system to visually communicate the current integration status of each domain:"
      },
      {
        "type": "text",
        "value": "üü© Green: Fully integrated and available via the data mart<br>üü® Yellow: In progress<br>‚¨ú Gray: Not yet available"
      },
      {
        "type": "text",
        "value": "„Ö§„Ö§This domain overview helped align stakeholders and clarify which systems had been onboarded into our centralized database, ready for BAU (Business-As-Usual) access."
      },
      {
        "type": "text",
        "value": "„Ö§„Ö§Recognizing the scale and impact of the data fragmentation, I set out to build an end-to-end data infrastructure‚Äîfrom ingestion, transformation, and data modeling to reporting and monitoring. My objective was to deliver:"
      },
      {
        "type": "text",
        "value": "„Ö§„Ö§‚óè A scalable, automated data pipeline <br> „Ö§„Ö§‚óè Clean, reliable data for business teams <br> „Ö§„Ö§‚óè Reduced manual data wrangling <br> „Ö§„Ö§‚óè Accelerated time-to-insight for decision-makers"
      },
      {
        "type": "text",
        "value": "This foundational work laid the groundwork for the subsequent project titled (End-to-End Data Infrastructure Planning), which formalized these efforts into a sustainable data engineering framework."
      },
      {
        "type": "image",
        "value": "/images/data-en-1.jpg",
        "caption": "Domain"
      },
      {
        "type": "text",
        "value": "<strong>Phase 1: Building from Scratch ‚Äì Ubuntu Server & Database Foundation </strong> <br><br>„Ö§„Ö§The project began with a blank slate. I collaborated closely with the Infrastructure team to provision and configure an on-premises Ubuntu Server. This phase involved setting up core system configurations, including: <br>„Ö§„Ö§‚óè Network and Security Setup: Configuring firewall rules, SSH access, and port forwarding<br>„Ö§„Ö§‚óè OS Installation: Installing and hardening the Ubuntu Server (Linux) environment <br>„Ö§„Ö§‚óè Backup Policy Compliance: Aligning with organizational standards for data backup and recovery procedures <br><br>„Ö§„Ö§To organize system services effectively, we adopted a container-based architecture using Portainer, which allowed us to manage and segment different stacks of services in a modular and maintainable way."
      },
      {
        "type": "text",
        "value": "„Ö§„Ö§Database Setup As a foundational layer, we deployed a MySQL relational database to handle raw and transactional data ingestion. MySQL was selected based on its performance efficiency for high-frequency transaction logging‚Äîespecially suitable for IoT-originated datasets such as sensor feeds and device logs."
      },
      {
        "type": "text",
        "value": "„Ö§„Ö§This database served as the Raw / Staging Layer, capturing incoming data streams for further processing."
      },
      {
        "type": "text",
        "value": "„Ö§„Ö§To facilitate schema design, database maintenance, and query execution, I used DBeaver, a powerful database GUI tool, to perform:<br>„Ö§„Ö§‚óè DDL scripting (table/view creation, indexing)<br>„Ö§„Ö§‚óè SQL query writing for exploration and validation<br>„Ö§„Ö§‚óè Connection management across different environments (Dev/Test/Prod)<br><br>This initial phase laid the groundwork for the subsequent development of ETL pipelines and data-transformation workflows in the next phase."
      },
      {
        "type": "image",
        "value": "/images/data-en-3.png",
        "caption": "Example.png"
      },
      {
        "type": "text",
        "value": "<strong>Phase 2: Designing a Scalable ETL Pipeline with Windmill</strong><br><br>„Ö§„Ö§To establish a reliable and maintainable data pipeline, I chose Windmill as the orchestration tool for building ETL workflows. Given the current system‚Äôs moderate complexity and relatively low data volume, Windmill offered an ideal balance between functionality and simplicity. Its intuitive UI/UX and low operational overhead made it a pragmatic choice over more complex orchestrators such as Apache Airflow, which is better suited for large-scale systems and may be considered for future expansion."
      },
      {
        "type": "image",
        "value": "/images/data-en-5.png",
        "caption": "Example.png"
      },
      {
        "type": "text",
        "value": "       Rationale Behind Tool Selection<br>„Ö§„Ö§‚óè Low learning curve for IT and Dev teams with limited experience in complex orchestration tools<br>„Ö§„Ö§‚óè Efficient UI for managing scripts, scheduling, and variables<br>„Ö§„Ö§‚óè Built-in integrations and error tracking, which reduce setup time for small- to mid-sized data workflows"
      },
      {
        "type": "text",
        "value": "ETL Architecture Overview<br>The pipeline was modularized into three core stages:<br>„Ö§„Ö§Extract: Ingest data from internal and external systems (e.g., APIs, IoT devices, vendor systems)<br>„Ö§„Ö§Transform: Clean, normalize, deduplicate, and enrich data to meet business-logic requirements<br>„Ö§„Ö§Load: Push the processed data into the target relational database, structured for downstream analytics"
      },
      {
        "type": "text",
        "value": "„Ö§„Ö§‚óè Extract<br>Data extraction involved both technical and organizational challenges. Each integration required:<br>„Ö§„Ö§„Ö§„Ö§- Engaging Data Owners to gain consent and clarify business rationale<br>„Ö§„Ö§„Ö§„Ö§- Collaborating with internal and external stakeholders, including vendors<br>„Ö§„Ö§„Ö§„Ö§- Implementing technical connectors via RESTful APIs, webhooks, or file-based ingestion<br>„Ö§„Ö§„Ö§„Ö§- Handling network configurations and authentication, often in partnership with the infrastructure team"
      },
      {
        "type": "text",
        "value": "„Ö§„Ö§‚óè Transform<br>Transformations ranged from simple data formatting to more complex logic encoding business rules. Since our end users consumed data primarily via Power BI, the goal was to minimize complex DAX expressions and instead deliver analysis-ready data via a curated Data Mart layer.<br><br>This ensured:<br>„Ö§„Ö§„Ö§„Ö§- Faster report development<br>„Ö§„Ö§„Ö§„Ö§- Improved data literacy among business users<br>„Ö§„Ö§„Ö§„Ö§- Reduced dependency on BI developers for logic implementation"
      },
      {
        "type": "text",
        "value": "„Ö§„Ö§‚óè Load<br>The final phase included various data-loading strategies, depending on use case:<br>„Ö§„Ö§„Ö§„Ö§- Full replacement for static datasets<br>„Ö§„Ö§„Ö§„Ö§- Incremental updates using timestamp logic or unique keys<br>„Ö§„Ö§„Ö§„Ö§- Chunk/batch processing for large datasets to avoid memory bottlenecks<br><br>Each script was modularized by purpose to support scalability and debugging‚Äîfor example:<br>„Ö§„Ö§‚óè Full refresh<br>„Ö§„Ö§‚óè Point updates<br>„Ö§„Ö§‚óè Append-only loads"
      },
      {
        "type": "image",
        "value": "/images/data-en-4.jpg",
        "caption": "Example.png"
      },
      {
        "type": "text",
        "value": "<strong>Scheduler & Monitoring</strong><br>Windmill‚Äôs built-in scheduling (via cron syntax) enabled automatic execution of pipelines at specified intervals. Additionally, I implemented error notifications via email to ensure that any pipeline failures could be quickly identified and resolved by the team."
      },
      {
        "type": "text",
        "value": "<strong>Security & Credential Management</strong><br>To ensure secure and auditable workflows:<br>„Ö§„Ö§‚óè IAM credentials and API keys were stored as environment variables and encrypted secrets<br>„Ö§„Ö§‚óè These were managed via Windmill‚Äôs secure vault, ensuring clear separation between business logic and sensitive configuration<br>„Ö§„Ö§‚óè This allowed safe collaboration without exposing privileged information in scripts or version control"
      },
      {
        "type": "image",
        "value": "/images/data-en-6.jpg",
        "caption": "Example.png"
      },
      {
        "type": "text",
        "value": "<strong>Phase 3: Building the Data Mart for Analytics Consumption</strong><br><br>Following data ingestion into the central database, the next critical step was the creation of curated Data Marts‚Äîpurpose-built datasets tailored to the analytical and reporting needs of various business units."
      },
      {
        "type": "text",
        "value": "These data marts were designed to serve as the semantic layer for self-service BI tools, reducing the need for business users to query raw tables or write complex transformation logic."
      },
      {
        "type": "text",
        "value": "<strong>Governance & Security Considerations</strong> <br>This phase required careful attention to two key areas:<br>1. Data Governance<br>2. Visualization & Insight Delivery (via Power BI)"
      },
      {
        "type": "text",
        "value": "Given that some source data contained Personally Identifiable Information (PII), it was essential to adhere to data privacy best practices. My approach included:<br><br>„Ö§„Ö§- Segregation of Sensitive Data: All PII was isolated from analytical layers<br>„Ö§„Ö§- Anonymization & Encoding: Sensitive attributes were transformed using techniques such as one-hot encoding, ensuring analytical utility without exposing identity<br>„Ö§„Ö§- Marking and Classification: Governance tags were applied for audit and policy enforcement (as part of a future expansion)"
      },
      {
        "type": "text",
        "value": "As a result, the finalized Data Marts were secure, anonymized, and analytics-ready, ensuring compliance and usability."
      },
      {
        "type": "text",
        "value": "Architecture & Performance Design<br>Using Windmill, I structured the transformation pipeline as modular and asynchronous flows, allowing for:<br><br>„Ö§„Ö§- Parallel Processing: Independent transformation blocks executed concurrently to reduce overall ETL runtime<br>„Ö§„Ö§- Nested Sub-flows: Each task in the flow could call sub-flows to encapsulate reusable logic (e.g., shared cleaning steps or lookups)<br>„Ö§„Ö§- Reduced Complexity: Visual flow diagrams improved maintainability and onboarding for new team members"
      },
      {
        "type": "text",
        "value": "This design not only increased pipeline efficiency but also provided clear visualization of each step in the data flow, from staging through to mart population."
      }      ,
      {
        "type": "text",
        "value": "BI Integration<br>The Data Mart output served as the primary data source for Power BI dashboards, which I designed to:<br>„Ö§„Ö§- Simplify user experience (minimize reliance on complex DAX logic)<br>„Ö§„Ö§- Support drill-down, trend analysis, and KPI tracking<br>„Ö§„Ö§- Enable data-driven decision-making across departments"
      },
      {
        "type": "text",
        "value": "By ensuring that data delivered to Power BI was clean, enriched, and pre-aggregated, we significantly reduced time-to-insight and empowered business users to explore data confidently."
      },
      {
        "type": "image",
        "value": "/images/data-en-7.jpg",
        "caption": "Example.png"
      },
      {
        "type": "text",
        "value": "<strong>Phase 4: Visualization & Insight Delivery</strong><br><br>The final phase focused on enabling business users to independently access, explore, and derive value from data. This phase was separated into two core initiatives:<br><br>- Dashboard & Monitoring<br>- Reporting & Advanced Analytics"
      },
      {
        "type": "text",
        "value": "„Ö§„Ö§These efforts were part of a broader initiative under the project \"[Data-Driven]\", designed to empower BAU (Business-As-Usual) teams to leverage BI tools effectively and reduce operational dependency on the Business Analytics team."
      },
      {
        "type": "text",
        "value": "<strong>Dashboard & Monitoring</strong> <br>I connected the PostgreSQL Data Mart to Power BI, creating a suite of dashboards tailored to business needs. Key features included:<br>„Ö§„Ö§- Multi-dimensional analysis (daily, weekly, monthly trends)<br>„Ö§„Ö§- User-friendly design with clear navigation and visual cues<br>„Ö§„Ö§- Pre-built DAX measures for insights such as:<br>„Ö§„Ö§- Growth Rate comparisons<br>„Ö§„Ö§- Top-N performance by segment<br>„Ö§„Ö§- Year-over-year analysis"
      },
      {
        "type": "text",
        "value": "To support scalability and adoption:<br>„Ö§„Ö§- Dashboards were designed for low-code/no-code handoff to BAU departments<br>„Ö§„Ö§- Initial deployment included training and onboarding for designated users<br>„Ö§„Ö§- Long-term ownership was transitioned to each department, who could extend and modify dashboards based on evolving needs"
      },
      {
        "type": "text",
        "value": "Reporting & Deep-Dive Analysis<br><br>For more complex analytical needs, especially those requiring cross-domain joins or hypothesis testing beyond available Data Mart dimensions, responsibility remained with the Business Analytics team. These ‚Äúdeep reports‚Äù addressed:<br><br>„Ö§„Ö§- Ad hoc strategic questions<br>„Ö§„Ö§- Trend anomalies<br>„Ö§„Ö§- Exploratory insights for decision-makers"
      },
      {
        "type": "text",
        "value": "The backend pipeline supported this by integrating cleaned data from multiple domains into a semantic model within Power BI, enhancing accessibility and cross-functional analysis."
      },
      {
        "type": "text",
        "value": "<strong>Governance & Access Control</strong> <br>To ensure secure and organized usage:<br>„Ö§„Ö§- Each department was assigned a dedicated Power BI Workspace<br>„Ö§„Ö§- Role-based access was enforced via permission controls aligned with the organization‚Äôs Data Catalog<br>„Ö§„Ö§- Sensitive dashboards and datasets were isolated as needed to comply with internal governance standards"
      },
      {
        "type": "text",
        "value": "<strong>Summary</strong><br>By establishing a seamless connection from PostgreSQL to Power BI, I enabled departments to make data-driven decisions using self-service dashboards. These efforts not only improved data visibility but also:<br>„Ö§„Ö§- Reduced turnaround time for insights<br>„Ö§„Ö§- Offloaded routine reporting tasks from the analytics team<br>„Ö§„Ö§- Increased data literacy across business units"
      },
      {
        "type": "text",
        "value": "Note: The design, implementation process, and outcomes of selected dashboards are documented in a dedicated sub-project."
      }      ,
      {
        "type": "text",
        "value": "<strong>Challenges & Deep-Dive Learnings</strong><br>Throughout the course of this project, I encountered a wide range of technical and analytical challenges that significantly contributed to my professional growth. These real-world scenarios required both hands-on problem-solving and cross-domain collaboration. Key learning areas included:"
      },
      {
        "type": "text",
        "value": "<strong>System Configuration & Troubleshooting</strong><br>- Gained extensive experience working with Linux CLI, particularly in debugging environment setup and deployment errors.<br>- Resolved issues related to:<br>    - Misconfigured `YAML` files in Windmill and Docker stacks<br>    - Networking challenges within Portainer, ensuring proper container-to-container communication<br>    - Port conflicts and version compatibility across multiple services"
      },
      {
        "type": "text",
        "value": "<strong>ETL Pipeline Debugging</strong><br>- Investigated root causes of pipeline failures across the stack:<br>    - Source-side issues (e.g., data quality, schema mismatches)<br>    - Credential mismanagement<br>    - Execution-level errors within Windmill workflows<br>- Designed alert mechanisms and fallback strategies to reduce downtime and improve recovery time"
      },
      {
        "type": "text",
        "value": "<strong>Query & Workflow Optimization</strong><br>- Improved performance and reliability by:<br>    - Optimizing complex SQL queries and Python scripts<br>    - Redesigning pipeline logic to reduce memory and processing overhead<br>    - Implementing batch updates, error handling, and retry mechanisms for unstable data sources"
      },
      {
        "type": "text",
        "value": "<strong>Business Logic Reconstruction & Data Investigation</strong><br>„Ö§„Ö§- Dealt with legacy datasets stored without clear documentation<br>„Ö§„Ö§- Conducted reverse engineering of table relationships and column semantics to rediscover business logic<br>„Ö§„Ö§- Collaborated with stakeholders to reconstruct transformation rules and validate data accuracy before delivering to BI tools"
      },
      {
        "type": "text",
        "value": "These experiences deepened my ability to work across the full data lifecycle, from system-level configuration to business-impactful analytics, and strengthened my confidence in designing scalable, secure, and business-aligned data solutions."
      },
      {
        "type": "text",
        "value": "<strong>Final Outcome & Personal Reflection</strong><br>„Ö§„Ö§By the end of this project, I successfully delivered a functioning end-to-end data infrastructure that transformed fragmented, underutilized data into a structured, accessible, and business-ready asset. Key achievements included:"
      },
      {
        "type": "text",
        "value": "‚úÖ Deployment of an on-premises Ubuntu server with secure, containerized services using Portainer<br>‚úÖ Design and implementation of a modular ETL pipeline using Windmill to extract, clean, and load data into MySQL/PostgreSQL<br>‚úÖ Construction of data marts with clear separation of PII, aligned with data governance standards<br>‚úÖ Integration with Power BI, enabling departments to access self-service dashboards and perform actionable analysis<br>‚úÖ Reduction in manual reporting workload for the Business Analytics team through automation and empowerment of BAU users"
      },
      {
        "type": "text",
        "value": "Most importantly, this experience affirmed my passion for solving real-world problems with data‚Äîbalancing technical excellence with business value."
      }
    ]
  },
  "End-to-End Data Infrastructure Planning": {
    "title": "End-to-End Data Infrastructure Planning",
    "image": "/images/data-foundamatal-01.jpg",
    "tags": [
      "Data Architecture",
      "System Design",
      "Scalable Infrastructure",
      "Automation Workflow",
      "Metadata Management",
      "Security & Compliance",
      "Strategic Planning"
    ],
    "content": [
       {
        "type": "text",
        "value": "„Ö§„Ö§This diagram illustrates the foundational data architecture established during the initial phase of joining the MICE business, with a primary focus on Data Engineering. It outlines the planning and progressive development of key system components including the data ingestion pipeline, metadata governance, and storage layer. The visual representation also highlights how each component contributes to enabling long-term, data-driven decision-making that aligns with business goals."
      },
      {
        "type": "note",
        "value": "Work-in-progress diagram"
      },
      {
        "type": "image",
        "value": "/images/data-en-11.jpg",
        "caption": "Example.png"
      },
      {
        "type": "text",
        "value": "„Ö§„Ö§After establishing the overarching project objectives, the next step involved scoping the data domain to assess the current state of data availability‚Äîidentifying datasets that are available, not yet available, or in progress. This assessment serves as a foundation for prioritizing data pipelines and identifying critical data dependencies essential for analytics, governance, and downstream business applications."
      },
      {
        "type": "image",
        "value": "/images/data-en-1.jpg",
        "caption": "Example.png"
      },
      {
        "type": "text",
        "value": "<strong>System Architecture Design via Figma</strong><br>„Ö§„Ö§We leveraged Figma as a collaborative tool to design and iterate on the system‚Äôs data architecture. The visual-first approach allowed the Data Engineering team to clearly communicate structural concepts and data pipeline flows across the ingestion, processing, and serving layers. It also enabled cross-functional teams to better understand system interactions and actively contribute to the development process.<br>(Work-in-progress diagram)"
      },
      {
        "type": "image",
        "value": "/images/data-en-8.jpg",
        "caption": "Example.png"
      },
      {
        "type": "text",
        "value": "<strong>System Scope Overview</strong><br>„Ö§„Ö§The current system architecture is structured into six core layers, each with clearly defined responsibilities and supporting tools. This layered approach ensures scalability, governance, and interoperability across all components of the data platform."
      },
      {
        "type": "text",
        "value": "<strong>1. Governance & Monitoring Layer</strong><br>Purpose: Ensures data quality, security, access control, and overall governance throughout the platform."
      },
      {
        "type": "text",
        "value": "<strong>2. Pipeline & Operation Layer</strong><br>Purpose: Manages the ETL/ELT workflows and operational orchestration.<br>Key Tools: <br>„Ö§„Ö§2.1 Windmill: Orchestrates and manages pipeline execution with built-in version rollback<br>„Ö§„Ö§2.2 GitLab: Used for version control (Windmill is prioritized due to native version tracking)<br>Supports scheduling and dependency management within pipelines"
      },
      {
        "type": "text",
        "value": "<strong>3. Database & Storage Layer</strong><br>Purpose: Stores both structured and unstructured data to support downstream processing and analytics.<br>Key Tools:<br>„Ö§„Ö§3.1 MySQL, PostgreSQL: Relational databases for structured data<br>„Ö§„Ö§3.2 MinIO: Object storage for files and unstructured data"
      },
      {
        "type": "text",
        "value": "<strong>4. Tools & Application Layer</strong><br>Purpose: Purpose: Provides supporting tools for automation, analytics, and visualization.<br>Key Tools:<br>Stores both structured and unstructured data to support downstream processing and analytics.<br>Key Tools:<br>„Ö§„Ö§4.1 n8n: Workflow automation<br>„Ö§„Ö§4.2 Power BI, Google Analytics, Streamlit: Data visualization and dashboarding<br>„Ö§„Ö§4.3 OpenMetadata: Metadata management and lineage tracking"
      },
      {
        "type": "text",
        "value": "<strong>5. Cloud & IoT Application Layer</strong><br>Purpose: Purpose:Collects data from external applications and IoT devices in both batch and near real-time modes.<br>Data Ingestion:<br>Data enters through pre-configured gateways and message queues, enabling seamless flow into the pipeline layer"
      },
      {
        "type": "text",
        "value": "<strong>6. Client & Gateway Layer</strong><br>Purpose: Purpose: Serves as the interface for end-users and services to access the platform securely.<br>nd message queues,Access Points:<br>Kong: API Gateway for external client integrations<br>Azure AD: Internal access control and authentication"
      },
      {
        "type": "image",
        "value": "/images/data-en-9.jpg",
        "caption": "Example.png"
      },
      {
        "type": "text",
        "value": "This diagram provides an overview of the Data Platform architecture, comprising database structures, data pipelines, and storage systems designed to support both business operations and organizational analytics. The platform emphasizes scalability and is built to enable automation at the operational level."
      },
      {
        "type": "text",
        "title": "Database Layer",
        "value": "<strong>Database Layer</strong><br>Supports 6 data sources, including IoT systems and cloud-based applications<br> Stores a total of 18 tables in the MySQL database<br> hosting over 16 million records (approx. 4.5 GB) of structured data."
      },
      {
        "type": "text",
        "title": "Pipeline Layer",
        "value": "<strong>Pipeline Layer</strong><br>Operates with 18 automated data pipelines, supported by 12 worker instances<br> Handles an average of 4,255 scheduled jobs per day<br> Built to be scalable, enabling seamless daily data transfers and powering 10 connected dashboards."
      },
      {
        "type": "text",
        "title": "Storage Layer",
        "value": "<strong>Storage Layer</strong><br>Utilizes MinIO for object storage, primarily for image files and raw data objects<br>Contains over 700,000 objects for machine learning purposes (~1.19 GB(~1.19 GB)<br>Includes 2 dedicated datasets used for model training and AI development"
      },
      {
        "type": "text",
        "title": "Data Size Summary (Data at Rest)",
        "value": "<strong>Data Size Summary (Data at Rest)</strong><br>The stored data volume provides a snapshot of the system‚Äôs current capacity, covering both structured data and object storage. This data foundation is critical for enabling advanced analytics and machine learning model development."
      },
      {
        "type": "image",
        "value": "/images/data-en-10.png",
        "caption": "Example.png"
      },
      {
        "type": "text",
        "title": "Challenges in Designing and Planning a Scalable Data Platform",
        "value": "<strong>Challenges in Designing and Planning a Scalable Data Platform</strong><br>One of the key challenges in designing and planning a data platform is the ability to think beyond immediate requirements‚Äîensuring seamless system interoperability, supporting future scalability, and enabling non-disruptive upgrades<br><br>To address this, the system design must consider the following core dimensions:"
      },
      {
        "type": "text",
        "title": "System Compatibility",
        "value": "<strong>System Compatibility</strong><br>All modules must be able to communicate and operate together efficiently (inter-module compatibility)<br>Technologies should support open standards and offer easy API integrations to enable flexibility and extensibility"
      },
      {
        "type": "text",
        "title": "Operational Security",
        "value": "<strong>Operational Security</strong><br>Emphasis on secure pipeline execution and inter-service access control<br>Implementation of clear role-based access control (RBAC) to manage permissions and minimize risks"
      },
      {
        "type": "text",
        "title": "Configuration & System Tuning",
        "value": "<strong>Configuration & System Tuning</strong><br>The platform must be configurable and adaptable to changing workloads<br>Support system tuning for optimal performance across ingestion, processing, and serving stages"
      },
      {
        "type": "text",
        "title": "Configuration & System Tuning",
        "value": "<strong>End-to-End Data Security</strong><br>Data protection must be enforced at all stages:<br>-Data Ingestion: Input validation and sanitation<br>-In-Transit: Encrypted communication (e.g., TLS/SSL)<br>-At-Rest: Encrypted data storage (encryption at rest)<br>Maintain detailed audit logs and data lineage to ensure traceability and compliance"
      }
    ]
  },
  "Data Governance": {
    "title": "Data Governance",
    "image": "/images/data-gov-cover.jpg",
    "tags": [
      "Data Governance",
      "Metadata Management",
      "System Architecture Planning",
      "Policy & Compliance"
    ],
    "content": [
      {
        "type": "text",
        "value": "<strong>Data Governance Mindset & Structural Evolution</strong><br>At the early stage of building a data platform within the MICE business context, the foundational challenge centered around designing a sustainable and scalable Data Governance model that balanced security, flexibility, and business alignment."
      },
      {
        "type": "text",
        "value": "We explored three organizational data models before deciding on a Hybrid approach:<br> - Decentralized: Data responsibilities are scattered across departments, leading to autonomy but lack of control.<br> - Centralized: All data functions are consolidated under a central team, enabling tight governance but reducing agility.<br> - Hybrid (chosen model): Combines the strengths of both‚Äîcentral governance standards with decentralized execution at the business unit (BU) level."
      },
      {
        "type": "image",
        "value": "/images/data-gov-01.jpg",
        "caption": "Example.png"
      },
      {
        "type": "text",
        "value": "Key Principles of the Hybrid Governance Model"
      },
      {
        "type": "image",
        "value": "/images/data-gov-02.jpg",
        "caption": "Example.png"
      },
      {
        "type": "text",
        "value": "<strong>A New Organizational Mindset</strong><br>Implementing hybrid data governance is not only a technical transformation ‚Äî it‚Äôs a cultural shift. Key mindset shifts include:<br> - IT moves from ‚ÄúSupport Function‚Äù to Strategic Partner for Innovation<br> - Data is not just for technical staff ‚Äî everyone must become data literate <br> - Business Units own their data and drive their own outcomes, supported by tech governance"
      },
      {
        "type": "image",
        "value": "/images/data-gov-03.jpg",
        "caption": "Example.png"
      },
      {
        "type": "image",
        "value": "/images/data-gov-04.png",
        "caption": "Example.png"
      },
      {
        "type": "text",
        "value": "<strong>Implementation: OpenMetadata as a Governance Tool</strong><br>To operationalize our hybrid governance strategy, we adopted OpenMetadata as the core tool for managing metadata, ownership, lineage, and policy enforcement<br> - The implementation features included:<br>Mapping full lineage: Data Source ‚Üí Table ‚Üí Fields ‚Üí Dashboards<br> - Assigning data owners per table/column<br> - Visualizing lineage across pipelines and reporting layers<br> - Tagging sensitive data (e.g. PII, confidential) to automate compliance workflows<br>(Screenshots of implemented OpenMetadata UI were included in the documentation.)"
      },
      {
        "type": "text",
        "value": "<strong>Key Outcomes from This Approach</strong><br>By implementing a hybrid data governance framework and choosing the right tools, we achieved the following results:<br>1. Clear Data Ownership Across the Organization ‚Üí Every team understands which data they are responsible for and how to manage it<br>2. Reduced Redundancy and Improved Trust in Data ‚Üí With documentation, lineage, and shared standards, teams can confidently work with reliable data<br> 3. Future-Proof Architecture ‚Üí Designed to scale, while maintaining a consistent governance backbone<br> 4. Stronger Collaboration Between Data and Business Units ‚Üí Data initiatives are now co-owned and co-driven by both business and technical teams"
      }
           
    ]
  },
  "post-event analysis": {
    "title": "Post-Event Analysis",
    "image": "../images/project-feedback.png",
    "tags": [
      "Business Analysis",
      "Insight Communication",
      "Data Storytelling",
      "Dashboard Design & Visualization",
      "SQL for Data Extraction",
      "Strategic Thinking"
    ],
    "content": [
      {
        "type": "image",
        "value": "/images/Post-event-cover.jpg",
        "caption": "Example.png"
      },
      {
        "type": "text",
        "value": "<strong>In-Depth Post-Event Analysis</strong>"
      },
      {
        "type": "text",
        "value": "In the role of a Business Analytics professional, one of the core responsibilities is to analyze complex business issues‚Äîespecially in cases where business units (BUs) lack access to deep, actionable data. Our task is to bridge this gap through structured and insightful data analysis."
      },
      {
        "type": "text",
        "value": "In this project, the Marketing and Sales teams encountered significant challenges during the execution of a major event. After the event concluded, they compiled relevant data and passed it on to our team for a thorough analysis, with the goal of understanding what went wrong and identifying areas for improvement. Given the urgency and importance of the matter, we committed to delivering the analysis within two weeks."
      },
      {
        "type": "text",
        "value": "The first step involved exploring internal data from the Data Warehouse to assess what had occurred and uncover the root causes. However, the available internal data alone was insufficient to provide a complete picture. To address this, we expanded our scope through Cross-Domain Analysis, incorporating external public data sources to fill in the gaps and validate the initial hypotheses provided by the team."
      },
      {
        "type": "text",
        "value": "This analysis combined both quantitative and qualitative methods, integrating internal and external data to evaluate each issue's significance and ensure that conclusions were grounded in sound reasoning."
      },
      {
        "type": "text",
        "value": "The outcome of this Post-Event Analysis became a key component in enhancing decision-making for the Marketing and Sales teams. It also set a new internal standard‚Äîdemonstrating that conducting a post-event review is crucial for any high-stakes event, especially those contributing significantly to the company's revenue. This approach supports more informed planning and continuous improvement in event execution."
      },
      {
        "type": "image",
        "value": "/images/post-event-01.jpg",
        "caption": "Example.png"
      },
      {
        "type": "image",
        "value": "/images/post-event-02.jpg",
        "caption": "Example.png"
      },
      {
        "type": "text",
        "value": "<strong>Project Challenges</strong>"
      },
      {
        "type": "text",
        "value": "Complexity of Business and Data:<br>One of the primary challenges of this project was understanding the business context. The data involved was highly complex, and although it was possible to extract data directly from the system, it could not be used at face value. It required cross-referencing and combining with data from multiple sources to arrive at accurate and comprehensive conclusions."
      },
      {
        "type": "text",
        "value": "<strong>Designing Analysis for Clear Communication:</strong><br>Another key challenge was designing the analysis in a way that could be easily understood by business units (BUs), many of whom do not have a technical or data background. This required careful selection of tools, methods, and presentation techniques to ensure the insights were clear, actionable, and supportive of decision-making."
      }
    ]
  },
  "campaign-performance-analysis": {
    "title": "Content is comimg soon",
    "image": "../images/project-campaign.png",
    "tags": [
      "Marketing",
      "A/B Test",
      "Budget Optimization"
    ],
    "content": [
      {
        "type": "text",
        "value": "Analyzed campaign variations using A/B testing to inform budget decisions."
      }
    ]
  },
  "operational-kpi-tracking": {
    "title": "Operational KPI Tracking",
    "image": "../images/project-kpi.png",
    "tags": [
      "KPI",
      "Sales",
      "Service"
    ],
    "content": [
      {
        "type": "text",
        "value": "Created sales and service performance dashboards for internal tracking."
      }
    ]
  },
  "bi-sales-performance": {
    "title": "Routine Report Mice Business model",
    "image": "../images/bi-sales-dashboard.png",
    "tags": [
      "Power B",
      "Data Visualization",
      "Dashboard Design",
      "Business Intelligence",
      "Data-Driven Decision Making",
      "Stakeholder Communication"
    ],
    "content": [
      {
        "type": "image",
        "value": "/images/report-routine-01.jpg",
        "caption": "Example.png"
      },
      {
        "type": "text",
        "value": "As part of the organization‚Äôs transition toward a Data-Driven approach, empowering business users to access data and independently create reports using Power BI plays a key role. This not only improves agility and decision-making but also reduces reliance on manual work. This project was driven by two main objectives:<br>1. To promote a Data-Driven Organization<br>2. To reduce FTE (Full-Time Equivalent) allocated to routine reporting tasks"
      },
      {
        "type": "text",
        "value": "<strong>Project Background</strong><br>The Marketing team provided their existing manual weekly and monthly reports for review. Upon analysis, it was evident that the original reports combined multiple objectives into a single dataset, which led to confusion and inefficiencies in usage across departments."
      },
      {
        "type": "text",
        "value": "<strong>Approach and Framework</strong><br>To realign the report to actual needs, I adopted the 6W1H Framework (Who, What, When, Where, Why, Whom, How) to explore:<br>- Who uses this report regularly<br>- Who is involved in its creation and review- What decisions the report supports<br>- Which data points could be separated into raw vs. summary views<br>This structured discussion allowed us to redefine the reporting scope, streamline content, and tailor outputs based on specific user needs."
      },
      {
        "type": "text",
        "value": "<strong>Process and Outcomes</strong><br>The restructured report was rebuilt in Power BI to serve as an executive-level dashboard, providing an interactive and real-time overview of key metrics for management. Throughout the process, I worked closely with the Marketing team through scheduled review sessions. This ensured that any misalignment could be **cross-checked and resolved immediately, allowing us to maintain accuracy and transparency without having to wait for the final delivery."
      },
      {
        "type": "text",
        "value": "<strong>Final Deliverable (Dashboard Preview)</strong><br>(Insert screenshot or image of the completed dashboard here)"
      },
      {
        "type": "image",
        "value": "/images/report rooutine.png",
        "caption": "Example.png"
      },
      {
        "type": "text",
        "value": "<strong>Challenges</strong><br>One of the main challenges of this project was aligning and clarifying the purpose of the original dashboard. The existing reports had overlapping objectives, which caused confusion and inefficiencies across teams.Another key challenge was **designing multiple charts** in a way that maintains narrative continuity and visual consistency, allowing stakeholders to easily interpret the data and draw insights."
      },
      {
        "type": "text",
        "value": "<strong>Summary of Objectives</strong><br>1. To Promote a Data-Driven Organization<br>After delivering the first version of the dashboard to the Marketing (MKT) team, a dedicated Power BI Workspace was created for them. This enabled the MKT team to take ownership of their dashboards, make adjustments as needed, and even build additional reports independently.As a result, the workload of the business analytics team was significantly reduced, allowing them to focus on more strategic tasks."
      },
      {
        "type": "text",
        "value": "<strong>2. To Reduce FTE (Full-Time Equivalent) in Routine Work</strong><br>This initiative contributed to an estimated FTE cost reduction of 18,750 THB per year by automating repetitive reporting tasks previously handled manually by one staff member."
      }
    ]
  },
  "bi-customer-segmentation": {
    "title": "Smart Footfall Dashboard for Event Insight & Optimization",
    "image": "../images/bi-customer-segmentation.png",
    "tags": [
      "Segmentation",
      "Customer Behavior",
      "Analytics"
    ],
    "content": [
      {
        "type": "text",
        "value": "This project showcases the development of a real-time footfall dashboard tailored for a large-scale exhibition venue. The objective was to provide a comprehensive view of visitor traffic during a multi-day international trade event, using data from various integrated sources."
      },
      {
        "type": "text",
        "value": "<strong>Objectives</strong><br>1.Enhance data accessibility for operations and decision-making<br>2.Provide near-real-time event monitoring to key stakeholders<br>3.Compare performance metrics with historical events<br>3.Compare performance metrics with historical events<br>4.Improve operational efficiency across visitor management, parking, and staffing"
      },
      {
        "type": "image",
        "value": "/images/footfall-01.png",
        "caption": "Example.png"
      },
      {
        "type": "text",
        "value": "<strong>Features & Components</strong><br>Dashboard Modules:<br>-Visitor trends by date and time slot<br>-Entrance-based traffic heatmap<br>-Real-time parking occupancy<br>-Gender and age breakdown (via AI camera)<br>-Comparison to previous year‚Äôs event<br>Data Sources:<br>-IoT people counting sensors (hourly)<br>-AI-based CCTV analytics (every 15 minutes)<br>Event calendar metadata via public API<br>-Weather data integration<br>Automated refresh at multiple intervals throughout the day (e.g., 8 AM, 8.15 AM, 8.30 AM)"
      },
      {
        "type": "text",
        "value": "<strong>Key Challenges</strong><br>-Aligning metrics from multiple systems with different refresh rates<br>-Designing a seamless narrative through visuals to support various stakeholder needs<br>-Balancing near-real-time insights with performance (load time, responsiveness)"
      }
    ]
  },
  "bi-kpi-overview": {
    "title": "KPI Overview",
    "image": "../images/bi-kpi-overview.png",
    "tags": [
      "Executive",
      "Dashboard",
      "KPI"
    ],
    "content": [
      {
        "type": "text",
        "value": "Built summary dashboards for executives to monitor company-wide KPIs in real-time."
      }
    ]
  },
  "marketing-insight-framework": {
    "title": "Internal Tech Talk & Workshop",
    "image": "../images/data-strategy-marketing.png",
    "tags": [
      "Public Speaking",
      "Workshop Facilitation",
      "Employee Training",
      "Soft Skills Development"
    ],
    "content": [
      {
        "type": "image",
        "value": "/images/specker-03.jpg",
        "caption": "Example.png"
      },
      {
        "type": "text",
        "value": "<strong>Technology Trends & Adaptation in the AI Era</strong><br>I had the great opportunity to serve as a co-speaker and workshop facilitator in an internal training session focused on emerging technology trends ‚Äî both current innovations and those shaping the near future.<br>The session aimed to help employees understand:<br> - How technology is increasingly integrated into daily operations<br> - How to practically leverage these tools in the workplace<br> - How to practically leverage these tools in the workplace"
      },
      {
        "type": "text",
        "value": "This event was organized by the HR team and the Business Analytics department, as part of a knowledge-sharing initiative. Employees across departments were encouraged to share expertise and experiences, fostering a culture of learning and curiosity.<br>As part of this event, I:<br> - Shared my own research and insights in a digestible, engaging format<br> - Assisted in live demos to help colleagues visualize how emerging tools (especially AI-related) can be applied in real-world business scenarios<br> - Helped design a small workshop to ensure participants could interact, practice, and walk away with actionable ideas"
      },
      {
        "type": "text",
        "value": "<strong>Technology Trends & Adaptation in the AI Era</strong><br>I had the great opportunity to serve as a co-speaker and workshop facilitator in an internal training session focused on emerging technology trends ‚Äî both current innovations and those shaping the near future.<br>The session aimed to help employees understand:<br> - How technology is increasingly integrated into daily operations<br> - How to practically leverage these tools in the workplace<br> - How to practically leverage these tools in the workplace<br>It was incredibly rewarding to see the enthusiasm of participants and to contribute to a workplace culture that values continuous learning and tech-forward thinking."
      },
      {
        "type": "image",
        "value": "/images/specker-01.png",
        "caption": "Example.png"
      },
      {
        "type": "image",
        "value": "/images/specker-02.png",
        "caption": "Example.png"
      }
    ]
  },
  "event-analytics-dashboard": {
    "title": " Power BI Basic Training (Small Group Series)",
    "image": "../images/data-strategy-event.png",
    "tags": [
      "Public Speaking",
      "Workshop Facilitation",
      "Employee Training",
      "Soft Skills Development",
      "Power BI",
      "Hands-on Learning"
    ],
    "content": [
      {
        "type": "text",
        "value": "To support the organization's shift toward self-service analytics and data-driven decision-making, I facilitated a series of Power BI training sessions for small groups, segmented by department."
      },
      {
        "type": "text",
        "value": "<strong>Objective</storng>"
      },
      {
        "type": "text",
        "value": "The goal was to empower non-technical team members with fundamental Power BI skills, enabling them to:<br> - Understand the structure and logic of Power BI reports<br> - Navigate dashboards and interact with filters<br> - Perform basic data transformation using Power Query<br> - Create simple visualizations for their own team needs<br>This training was positioned as the first stage (Stage 1) of Power BI enablement across the company."
      },
      {
        "type": "text",
        "value": "<strong>Training Format</strong><br>Audience: Small groups (5‚Äì10 people per session) organized by department<br>Format: Hands-on workshop + live walkthrough<br>Duration: 2‚Äì3 hours per session<br>Tools used: Power BI Desktop, sample datasets, interactive exercises"
      },
      {
        "type": "text",
        "value": "<strong>My Role</strong><br>- Designed training flow and slide deck<br> - Led live demonstrations and step-by-step tutorials<br> - Tailored examples based on each department‚Äôs context<br> - Answered live questions and troubleshooted common beginner issues<br> - Followed up with support materials and links for self-paced learning"
      },
      {
        "type": "text",
        "value": "<strong>Impact</strong><br>- Helped multiple teams gain confidence in exploring and interpreting data<br> - Reduced dependency on centralized reporting teams for ad hoc data requests<br> - Established a scalable training structure for future advanced stages"
      },
      {
        "type": "image",
        "value": "/images/power-bi.png",
        "caption": "Example.png"
      },
      {
        "type": "image",
        "value": "/images/power-bi-01.png",
        "caption": "Example.png"
      }
    ]
  },
  "decision-intelligence-system": {
    "title": "Decision Intelligence System",
    "image": "../images/data-strategy-decision.png",
    "tags": [
      "AI",
      "Automation",
      "Decision Support"
    ],
    "content": [
      {
        "type": "text",
        "value": "Developed logic flows and automated actions triggered by analytical thresholds and forecasts."
      }
    ]
  },
  "chatbot-analytics-pipeline": {
    "title": "Chatbot Analytics Pipeline",
    "image": "../images/data-product-chatbot.png",
    "tags": [
      "Automation Design",
      "Web Scraping & Crawling",
      "Prompt Engineering",
      "Vector Database"
    ],
    "content": [
      {
        "type": "image",
        "value": "/images/Data_Illustration.jpg",
        "caption": "Example.png"
      },
      {
        "type": "text",
        "value": "AI-Powered Automation Workflow for Expo Website Support<br>Given the rapid advancement of technology‚Äîespecially in the fields of Automation and AI Agents‚Äîour team, which has been focused on establishing a strong Data Infrastructure, was presented with a challenge that pushed us to develop AI solutions in parallel."
      },
      {
        "type": "text",
        "value": "One of the key implementations was an AI-powered Chat Agent integrated into the Expo Website, designed to provide users with accurate information about the event while also handling unrelated questions gracefully to ensure the best possible user experience."
      },
      {
        "type": "text",
        "value": "</strong>Tools and Technologies Used</strong><br>We chose n8n as our main tool for designing the AI automation workflow. While n8n is primarily known as an automation platform, it offers powerful API integrations that allow it to work seamlessly with external AI models such as OpenAI (ChatGPT) and Gemini, making it flexible for LLM-based applications."
      },
      {
        "type": "text",
        "value": "Before reaching this stage, we ran a Proof of Concept (PoC) to validate the feasibility of AI agents under minimal cost, leveraging local infrastructure. This included deep exploration into:<br> - The architecture and mechanisms of Large Language Models (LLMs)<br> - The principles behind Retrieval-Augmented Generation (RAG)<br> - Hands-on practice with Langchain, FAISS, and various Vector Databases"
      },
      {
        "type": "text",
        "value": "AI Agent Workflow Design"
      },
      {
        "type": "text",
        "value": "Back-End Workflow: Vectorization & Retrieval<br>On the backend, we designed the workflow as follows:<br>1.Data Preparation<br>We collected data from various sources, including databases and documents, focusing on structured and semi-structured formats (excluding audio and image files).<br>2.PII Marking & Compliance<br>All data was reviewed and processed with PII (Personally Identifiable Information) marking to ensure compliance with data protection laws such as PDPA and GDPR.<br>3.Embedding & Indexing<br>Cleaned data was transformed into vector embeddings and stored in a vector database, such as FAISS, Qdrant, or ChromaDB.<br>5.RAG Pipeline<brUpon receiving a user query, the system performs a similarity search to retrieve the most relevant content from the vector store. This content is then merged with a prompt and passed to the LLM for response generation.<br>5.Response Filtering<br>The output from the LLM is screened through a sensitive content filter to prevent inappropriate or non-compliant responses before delivering it to the user"
      },
      {
        "type": "text",
        "value": "<strong>Front-End Workflow: User Interaction & Chat Engine</strong><br>1.User Input<br>User messages are received through the Chat UI, such as a chat box or embedded web widget.<br>2.Search & Re-Rank<br>The system performs both vector-based search and keyword-based search, followed by a re-ranking process to prioritize the most relevant results.<br>3.Safety Layer<br>A filtering layer is applied to block or deny queries that are considered sensitive, inappropriate, or out of defined scope.<br>4.Contextual Response Generation<br>Relevant results retrieved via the RAG process are passed to the LLM, which generates a context-aware response tailored to the user's intent.<br>5.Output Delivery<br>The final, filtered response is returned to the user through the chat interface.<br>The initial system draft illustrated a clear separation between the **Back-End AI Pipeline** and the **Front-End Chat Layer**, allowing for easier development, scalability, and future improvements.This draft was created **prior to the decision to adopt n8n** as the primary automation tool."
      },
      {
        "type": "image",
        "value": "/images/llm.png",
        "caption": "Example.png"
      },
      {
        "type": "image",
        "value": "/images/llm-2.png",
        "caption": "Example.png"
      },
      {
        "type": "text",
        "value": "All of these processes were implemented entirely within n8n.<br>After the initial Proof of Concept (PoC), our team expanded the project scope to support the **Exhibition Website**, with a refined focus on:.<br> - PII filtering and strict content controls were de-emphasized, as the source data was already publicly available.<br> - In contrast, Customer Experience became a primary focus, ensuring that responses were more accurate and aligned with user intent."
      },
      {
        "type": "text",
        "value": "The website is a static web page, and we applied web crawling techniques to automatically extract content and store it in a vector database. The process included:<br>1. Web Scraping: Extracting data from static web pages<br>2. Vectorization: Converting text into vector format for semantic search<br>3. Scope Control: Restricting the response generation to data exclusively from approved sources<br>4. LLM Response Generation: Using an LLM (Gemini) to craft responses based on the retrieved content<br>This entire workflow was built using n8n Workflow Automation, with no custom backend development required. The result is a low-cost, highly flexible solution that can be quickly adapted to different business contexts."
      },
      {
        "type": "image",
        "value": "/images/llm-3.png",
        "caption": "Example.png"
      },
      {
        "type": "text",
        "value": "Designed a Cost-Efficient AI System<br> - Planned a low-cost PoC using available local resources<br> - Built the entire automation workflow on n8n, with no need for custom backend development<br>Developed Vector-Based Search & AI Chat Agent<br> - Built an end-to-end pipeline: Web Scraping ‚Üí Embedding ‚Üí Vector Store ‚Üí AI Response<br> - Integrated with LLM to generate context-aware answers based on retrieved data<br>Systematic & Modular Design Thinking<br> - Designed a clear separation between Back-End (data retrieval + AI logic) and Front-End (chat interface)<br> - Ensured the system can be easily scaled or modularly replaced as needed in the future"
      }
    ]
  },
  "self-service-report-generator": {
    "title": "Self-Service Report Generator",
    "image": "../images/data-product-reportgen.png",
    "tags": [
      "SQL",
      "Power BI",
      "Automation"
    ],
    "content": [
      {
        "type": "text",
        "value": "Enabled internal users to generate reports dynamically from multiple databases using simple filters."
      }
    ]
  },
  "customer-feedback-hub": {
    "title": "Customer Feedback Hub",
    "image": "../images/data-product-feedback.png",
    "tags": [
      "Feedback",
      "Integration",
      "Customer Voice"
    ],
    "content": [
      {
        "type": "text",
        "value": "Centralized survey, support, and online reviews into a hub for product and service improvements."
      }
    ]
  },
"gcp-certification": {
  "title": "Google Cloud Certified Associate",
  "image": "../images/gcp-cert.png",
  "tags": ["GCP", "Cloud", "Infrastructure"],
  "content": [
    {
      "type": "text",
      "value": "Demonstrated foundational knowledge of Google Cloud services and architecture."
    },
    {
      "type": "image",
      "value": "../images/gcp-diagram.png",
      "caption": "GCP Service Overview"
    },
    {
      "type": "text",
      "value": "This certification affirms capability to manage cloud-native apps and services in GCP."
    }
  ]
},
"azure-certification": {
  "title": "Azure Fundamentals",
  "image": "../images/azure-cert.png",
  "tags": ["Azure", "Cloud", "Fundamentals"],
  "content": [
    {
      "type": "text",
      "value": "Certified on core Azure concepts, pricing models, and governance tools."
    },
    {
      "type": "image",
      "value": "../images/azure-overview.png",
      "caption": "Azure Cloud Foundations"
    },
    {
      "type": "text",
      "value": "This cert reflects understanding of Azure‚Äôs key services and cloud principles."
    }
  ]
},
"aws-certification": {
  "title": "AWS Solutions Architect Associate",
  "image": "../images/aws-cert.png",
  "tags": ["AWS", "Cloud", "Architecture"],
  "content": [
    {
      "type": "text",
      "value": "Designed scalable cloud solutions following AWS best practices."
    },
    {
      "type": "image",
      "value": "../images/aws-diagram.png",
      "caption": "AWS Architecture"
    },
    {
      "type": "text",
      "value": "This certification validates expertise in deploying secure cloud solutions.<br>-In-Transit: Encrypted communication (e.g., TLS/SSL)<br>At-Rest: Encrypted data storage (encryption at rest)<br>Maintain detailed audit logs and data lineage to ensure traceability and compliance"
    }
  ]
}
}